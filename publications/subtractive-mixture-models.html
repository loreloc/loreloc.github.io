<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>LL - Subtractive Mixture Models via Squaring: Representation and Learning</title>
        <link rel="icon" type="image/x-icon" href="../favicon.ico">
        <link rel="stylesheet" href="../css/default.css" />
        <script src="https://kit.fontawesome.com/6bd063083f.js" crossorigin="anonymous"></script>
        <script src="../js/events.js"></script>
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">Lorenzo Loconte</a>
            </div>
            <br />
        </header>

        <main role="main">
            <!--<h1>Subtractive Mixture Models via Squaring: Representation and Learning</h1>-->
            <article>
    <section class="header">Posted on January 16, 2024</section>

    <img class="spot" src="../static/publications/subtractive-mixture-models/spot.png" />

    <section>
        <h2>Subtractive Mixture Models via Squaring: Representation and Learning</h2>
        <b>L. Loconte</b>, A. M. Sladek, S. Mengel, M. Trapp, A. Solin, N. Gillis, A. Vergari.
        <br />
        ICLR 2024. <strong>Spotlight (top 5%)</strong>.
    </section>

    <section class="publinks">
        <a class="pubbutton" href="https://openreview.net/forum?id=xIHi5nxu9P">Paper</a>
        <a class="pubbutton" href="../static/publications/subtractive-mixture-models/poster.pdf">Poster</a>
        
        <a class="pubbutton" href="../static/publications/subtractive-mixture-models/slides.pdf">Slides</a>
        
            <button class="pubbutton" onclick="copyFileToClipboard('/static/publications/subtractive-mixture-models/loconte2023subtractive.bib')">
                <i class="fa-regular fa-copy" id="ibibcopy"></i> BibTeX
            </button>
        
    </section>

    <section class="abstract">
        <h2 style="text-align: center;">Abstract</h2>
        Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
    </section>

    <br />
</article>

        </main>

        <footer>
            Site proudly generated by
            <a href="http://jaspervdj.be/hakyll">Hakyll</a>
        </footer>
    </body>
</html>
